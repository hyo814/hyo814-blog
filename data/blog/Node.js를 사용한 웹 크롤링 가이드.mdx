title: Node를 사용한 웹 크롤링 가이드
date: '2024-06-21'
tags: [Node, crawling]
draft: false
summary: 'crawling'

---

# Node.js를 사용한 웹 크롤링 가이드

### Node.js를 사용한 웹 크롤링 가이드

웹 크롤링은 웹 페이지에서 필요한 데이터를 자동으로 수집하는 프로세스로, 가격 비교, 뉴스 수집, 데이터 분석 등의 다양한 용도로 활용됩니다. 이 글에서는 **Node.js**를 사용해 간단한 웹 크롤러를 만드는 방법을 설명하겠습니다. 크롤러는 웹 페이지에서 데이터를 추출하고, 이를 파일에 저장할 수 있는 유용한 도구입니다. Node.js와 함께 사용할 **axios**와 **cheerio** 라이브러리를 활용하여, 간단한 크롤러를 단계별로 구현해 보겠습니다.

---

### 준비물

1. **Node.js**: 비동기 이벤트 기반 JavaScript 런타임 환경.
2. **axios**: HTTP 요청을 보내고 응답을 받기 위한 라이브러리.
3. **cheerio**: 서버에서 HTML을 파싱하고 DOM을 조작하기 위한 라이브러리.

---

### 1단계: Node.js 설치

Node.js가 설치되어 있지 않다면 [Node.js 공식 웹사이트](https://nodejs.org/)에서 최신 버전을 설치하세요. 설치 후, 터미널에서 `node -v` 명령어를 입력해 설치가 완료되었는지 확인합니다.

```bash
node -v

```

---

### 2단계: 프로젝트 초기화 및 필수 라이브러리 설치

크롤러 프로젝트를 위한 디렉토리를 생성한 후, 터미널에서 `npm init` 명령어로 프로젝트를 초기화합니다.

```bash
mkdir web-crawler
cd web-crawler
npm init -y

```

다음으로, **axios**와 **cheerio** 라이브러리를 설치합니다.

```bash
npm install axios cheerio

```

- **axios**: HTTP 요청을 처리해주는 라이브러리.
- **cheerio**: HTML을 파싱하고 DOM을 조작하는 데 사용되는 라이브러리.

---

### 3단계: 기본 크롤러 코드 작성

이제 **index.js** 파일을 생성하고, 기본적인 웹 크롤러 코드를 작성해 보겠습니다. 여기서는 `axios`로 웹 페이지 데이터를 가져오고, `cheerio`로 HTML을 파싱하여 페이지 제목을 추출합니다.

```jsx
// 필요한 라이브러리 불러오기
const axios = require('axios')
const cheerio = require('cheerio')

// 크롤링할 URL 설정
const url = '<https://example.com>'

// 비동기 함수 정의
async function crawl() {
  try {
    // axios를 사용하여 페이지 가져오기
    const response = await axios.get(url)
    const html = response.data

    // cheerio를 사용하여 HTML 파싱
    const $ = cheerio.load(html)

    // 원하는 데이터 추출 (페이지 제목)
    const title = $('title').text()

    console.log(`페이지 제목: ${title}`)
  } catch (error) {
    console.error(`크롤링 실패: ${error.message}`)
  }
}

// 크롤링 함수 실행
crawl()
```

이 코드는 [**example.com**](http://example.com/) 웹사이트에서 페이지 제목을 추출합니다. `axios`를 통해 HTML 데이터를 가져오고, `cheerio`로 DOM을 파싱하여 데이터를 추출합니다.

---

### 4단계: 특정 데이터 추출

이제 특정한 데이터를 추출하는 방법을 알아보겠습니다. 예를 들어, 블로그 사이트에서 게시물의 제목과 URL을 추출하는 크롤러를 구현할 수 있습니다.

```jsx
const url = '<https://example-blog.com>'

async function crawl() {
  try {
    const response = await axios.get(url)
    const html = response.data
    const $ = cheerio.load(html)

    // 블로그 게시물의 제목과 URL 추출
    $('article.post').each((index, element) => {
      const title = $(element).find('h2').text()
      const link = $(element).find('a').attr('href')
      console.log(`제목: ${title}`)
      console.log(`링크: ${link}`)
    })
  } catch (error) {
    console.error(`크롤링 실패: ${error.message}`)
  }
}

crawl()
```

이 코드는 [**example-blog.com**](http://example-blog.com/)에서 게시물 제목과 링크를 추출하여 콘솔에 출력합니다. `cheerio`를 사용해 특정 HTML 요소를 찾아 데이터를 추출합니다.

---

### 5단계: 데이터 저장

크롤링한 데이터를 JSON 파일에 저장할 수 있습니다. 다음 코드는 게시물의 제목과 링크를 추출한 후, 이를 **posts.json** 파일에 저장합니다.

```jsx
const fs = require('fs')
const url = '<https://example-blog.com>'

async function crawl() {
  try {
    const response = await axios.get(url)
    const html = response.data
    const $ = cheerio.load(html)

    let posts = []

    $('article.post').each((index, element) => {
      const title = $(element).find('h2').text()
      const link = $(element).find('a').attr('href')
      posts.push({ title, link })
    })

    // JSON 파일로 저장
    fs.writeFileSync('posts.json', JSON.stringify(posts, null, 2))
    console.log('데이터가 성공적으로 저장되었습니다.')
  } catch (error) {
    console.error(`크롤링 실패: ${error.message}`)
  }
}

crawl()
```

이 코드는 블로그에서 게시물 정보를 추출한 후, **posts.json** 파일에 저장합니다. 이렇게 저장한 데이터는 다양한 분석이나 활용을 위해 사용할 수 있습니다.

---

### 결론

이 가이드에서는 **Node.js**를 사용한 기본적인 웹 크롤링 방법을 배웠습니다. **axios**와 **cheerio** 라이브러리를 사용하여 웹 페이지에서 데이터를 가져오고, 특정 데이터를 추출한 후, 이를 파일로 저장하는 과정을 단계별로 설명했습니다.

- **axios**: HTTP 요청을 처리하여 웹 페이지의 HTML 데이터를 가져오는 데 사용됩니다.
- **cheerio**: 가져온 HTML 데이터를 파싱하여 DOM을 쉽게 탐색하고, 원하는 데이터를 추출하는 데 사용됩니다.
- **fs**: 추출한 데이터를 파일로 저장하기 위한 Node.js의 파일 시스템 모듈입니다.

크롤링할 때는 **사이트의 이용 약관**을 준수하고, **과도한 트래픽**을 발생시키지 않도록 주의해야 합니다. 더 나아가 크롤링할 페이지 구조에 맞춰 코드를 수정하고, 데이터베이스에 저장하거나 다양한 데이터를 수집하는 고급 크롤러로 확장할 수 있습니다.

### 웹 크롤러를 만들 때 서버의 응답 속도가 느린 경우 어떻게 대처할 수 있을까요?

서버의 응답 속도가 느릴 경우 웹 크롤러의 성능을 높이기 위한 여러 가지 방법이 있습니다.

1. **타임아웃 설정**: `axios`나 다른 HTTP 요청 라이브러리에서 요청 타임아웃을 설정할 수 있습니다. 서버가 너무 오랫동안 응답하지 않으면 타임아웃을 설정하여 요청을 취소하고 다음 작업을 처리할 수 있습니다.

   ```jsx
   const axios = require('axios')

   axios
     .get('<https://example.com>', { timeout: 5000 }) // 5초 타임아웃
     .then((response) => {
       console.log(response.data)
     })
     .catch((error) => {
       console.error('타임아웃 발생:', error.message)
     })
   ```

2. **재시도 메커니즘**: 요청이 실패할 경우 자동으로 다시 시도하는 기능을 구현할 수 있습니다. 이를 통해 네트워크 문제나 서버의 일시적인 과부하로 인한 실패를 복구할 수 있습니다.

   ```jsx
   const axiosRetry = require('axios-retry')

   axiosRetry(axios, { retries: 3 }) // 3번 재시도

   axios
     .get('<https://example.com>')
     .then((response) => {
       console.log(response.data)
     })
     .catch((error) => {
       console.error('재시도 후에도 실패:', error.message)
     })
   ```

3. **병렬 요청**: 여러 페이지를 동시에 크롤링할 경우 병렬 요청을 보내 성능을 향상시킬 수 있습니다. `Promise.all`을 사용하여 병렬로 HTTP 요청을 처리하고, 전체 완료 시간을 줄일 수 있습니다.

   ```jsx
   const urls = ['<https://example.com/page1>', '<https://example.com/page2>']

   const fetchPages = urls.map((url) => axios.get(url))

   Promise.all(fetchPages)
     .then((responses) => {
       responses.forEach((response) => {
         console.log(response.data)
       })
     })
     .catch((error) => {
       console.error('병렬 요청 중 실패:', error.message)
     })
   ```

4. **캐싱 사용**: 동일한 요청을 반복적으로 하는 경우, 캐시를 활용해 동일한 데이터를 여러 번 요청하지 않도록 최적화할 수 있습니다.

### cheerio와 비슷한 다른 HTML 파싱 라이브러리에는 어떤 것들이 있나요?

**Cheerio**는 서버 사이드에서 HTML을 파싱하고 DOM을 조작하는 데 자주 사용되는 라이브러리지만, 비슷한 기능을 제공하는 다른 라이브러리들도 많이 있습니다.

1. **Puppeteer**:
   - Puppeteer는 Google에서 개발한 헤드리스 브라우저로, 브라우저에서 HTML 페이지를 렌더링한 후 DOM을 조작할 수 있습니다. Cheerio와 달리 브라우저 환경을 완전히 시뮬레이션할 수 있어, 자바스크립트로 렌더링된 동적 페이지에도 접근이 가능합니다.
   - **사용 사례**: 자바스크립트로 렌더링되는 SPA(Single Page Application)에서 데이터 크롤링.
   - `const puppeteer = require('puppeteer');
(async () => { const browser = await puppeteer.launch(); const page = await browser.newPage(); await page.goto('<https://example.com>'); const title = await page.title(); console.log(title); await browser.close();
})();`
2. **JSDOM**:
   - JSDOM은 완전한 DOM 환경을 제공하는 Node.js 라이브러리로, 브라우저에서 사용하는 DOM API와 유사한 환경을 제공합니다. Cheerio와 비교해 더 완전한 DOM 기능을 제공하며, 자바스크립트 실행도 지원합니다.
   - **사용 사례**: 브라우저와 비슷한 DOM API가 필요한 경우.
   - `const jsdom = require('jsdom');
const { JSDOM } = jsdom;
const dom = new JSDOM(`<html><body><h1>Hello World</h1></body></html>`);
console.log(dom.window.document.querySelector('h1').textContent);`
3. **Axios + JSDOM**:
   - Cheerio처럼 간단하게 HTML 파싱을 원하면서도, JSDOM을 통해 브라우저에서 동작하는 방식에 더 가깝게 처리하고 싶다면 Axios와 JSDOM을 함께 사용할 수 있습니다.
   - `const axios = require('axios');
const jsdom = require('jsdom');
const { JSDOM } = jsdom;
axios.get('<https://example.com>') .then(response => { const dom = new JSDOM(response.data); console.log(dom.window.document.querySelector('h1').textContent); });`
4. **Playwright**:
   - Puppeteer와 유사한 헤드리스 브라우저로, 여러 브라우저(Chrome, Firefox, WebKit)를 자동화할 수 있습니다. 브라우저 자동화가 필요할 때 매우 유용하며, Playwright는 Puppeteer보다 더 많은 브라우저 호환성을 제공합니다.
   - **사용 사례**: 다양한 브라우저 환경에서 테스트하거나 크롤링할 때.
   - `const playwright = require('playwright');
(async () => { const browser = await playwright.chromium.launch(); const page = await browser.newPage(); await page.goto('<https://example.com>'); const title = await page.title(); console.log(title); await browser.close();
})();`

### 크롤링할 때 발생할 수 있는 법적 문제는 무엇이 있으며, 이를 방지하기 위한 방법은 무엇인가요?

웹 크롤링을 할 때는 법적 이슈를 반드시 고려해야 합니다. 웹 사이트의 컨텐츠는 저작권법, 서비스 약관, 그리고 기타 법률에 의해 보호될 수 있기 때문입니다. 크롤링을 하기 전에 다음 사항들을 숙지하는 것이 중요합니다.

1. **사이트의 이용 약관(TOS)**:
   - 대부분의 웹사이트는 크롤링이나 스크래핑을 금지하는 조항을 서비스 약관에 명시해 놓습니다. 사이트의 약관을 무시하고 크롤링을 진행하면 법적 문제가 될 수 있습니다. 크롤링하기 전에 반드시 해당 사이트의 약관을 읽고 크롤링 가능 여부를 확인해야 합니다.
2. **로봇 배제 표준(robots.txt)**:

   - `robots.txt` 파일은 사이트 소유자가 웹 크롤러가 접근할 수 있는 페이지와 접근할 수 없는 페이지를 정의하는 파일입니다. 법적 구속력이 있지는 않지만, 사이트 관리자가 허용하지 않은 페이지를 크롤링할 경우 법적 문제로 이어질 수 있습니다.
   - **예시**: `https://example.com/robots.txt`를 확인하여 크롤링이 허용된 영역만 크롤링합니다.

   ```
   User-agent: *
   Disallow: /private/

   ```

3. **저작권 침해**:
   - 웹 페이지의 내용은 저작권법에 의해 보호될 수 있습니다. 특히 뉴스 기사, 이미지, 글 등의 콘텐츠를 무단으로 수집하고 재배포하는 것은 저작권 침해에 해당할 수 있습니다. 데이터를 수집할 때는 원저작자의 권리를 존중해야 하며, 이를 상업적으로 사용하기 전에 반드시 허가를 받아야 합니다.
4. **사이트에 과도한 트래픽 유발**:

   - 지나치게 빈번한 요청으로 서버에 과부하를 유발하면, 서비스 거부(DoS) 공격으로 간주될 수 있습니다. 이를 방지하기 위해 크롤러에 적절한 **딜레이**를 두거나, **요청 빈도를 제한**하여 서버에 과도한 부담을 주지 않도록 해야 합니다.

   ```jsx
   function delay(ms) {
     return new Promise((resolve) => setTimeout(resolve, ms))
   }

   async function crawlWithDelay() {
     const urls = ['<https://example.com/page1>', '<https://example.com/page2>']

     for (const url of urls) {
       await axios.get(url)
       await delay(2000) // 2초 딜레이
     }
   }

   crawlWithDelay()
   ```

5. **API 사용 권장**:
   - 많은 웹사이트는 데이터를 제공하기 위한 공식 API를 제공합니다. 공식 API를 사용하면 사이트의 정책을 준수하면서도 데이터를 안전하게 수집할 수 있습니다. 크롤링 대신 API를 사용하는 것이 법적 리스크를 줄이는 좋은 방법입니다.
